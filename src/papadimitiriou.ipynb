{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from resnet_cifar import *\n",
    "from wide_resnet_cifar import *\n",
    "from resnext_cifar import *\n",
    "from densenet_cifar import *\n",
    "from custom_loader_cifar import *\n",
    "from orig_cifar import *\n",
    "\n",
    "import parser\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, \n",
    "                 epoch=200, \n",
    "                 batch_size=128, lr=0.1, momentum=0.9, wd=1e-4, print_freq=10, resume=\"\", evaluate=True, ct=10):\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.wd = wd\n",
    "        self.print_freq = print_freq\n",
    "        self.resume = resume\n",
    "        self.evaluate = evaluate\n",
    "        self.ct = ct\n",
    "        self.weight_decay = wd\n",
    "        self.cifar_type = 10\n",
    "\n",
    "def main1(epoch=200, batch_size=128, lr=0.1, momentum=0.9, wd=1e-4, print_freq=10, resume=\"\", evaluate=False, ct=10):\n",
    "    \n",
    "    global args, best_prec\n",
    "    best_prec = 0\n",
    "    args = Parameters()\n",
    "    #args = parser.parse_args()\n",
    "    args.epoch = epoch\n",
    "    args.batch_size = batch_size\n",
    "    args.lr = lr\n",
    "    args.momentum = momentum\n",
    "    args.wd = wd\n",
    "    args.print_freq = print_freq\n",
    "    args.resume = resume\n",
    "    args.evaluate = evaluate\n",
    "    args.ct = ct\n",
    "    args.start_epoch = 0\n",
    "    args.epochs = epoch\n",
    "    \n",
    "    #print(\"Hello\")\n",
    "    main2()\n",
    "    \n",
    "\n",
    "def main2():\n",
    "    global args, best_prec\n",
    "    #print(\"Hello1\")\n",
    "    '''\n",
    "    epoch=200, batch_size=128, lr=0.1, momentum=0.9, wd=1e-4, print_freq=10, resume=\"\", evaluate=True, ct=10\n",
    "    epoch=160, batch_size=128, lr=0.1, momentum=0.9, wd=1e-4, ct=10\n",
    "    global args, best_prec\n",
    "    args = parser.parse_args()\n",
    "    args.epoch = epoch\n",
    "    args.batch_size = batch_size\n",
    "    args.lr = lr\n",
    "    args.momentum = momentum\n",
    "    args.wd = wd\n",
    "    args.print_freq = print_freq\n",
    "    args.resume = resume\n",
    "    args.evaluate = evaluate\n",
    "    args.ct = ct\n",
    "    '''\n",
    "    \n",
    "    #global args, best_prec\n",
    "    #args = parser.parse_args()\n",
    "    #use_gpu = torch.cuda.is_available()\n",
    "    use_gpu = False\n",
    "\n",
    "    # Model building\n",
    "    print('=> Building model...')\n",
    "    if use_gpu or True:\n",
    "        # model can be set to anyone that I have defined in models folder\n",
    "        # note the model should match to the cifar type !\n",
    "\n",
    "        model = resnet20_cifar()\n",
    "        # model = resnet32_cifar()\n",
    "        # model = resnet44_cifar()\n",
    "        # model = resnet110_cifar()\n",
    "        # model = preact_resnet110_cifar()\n",
    "        # model = resnet164_cifar(num_classes=100)\n",
    "        # model = resnet1001_cifar(num_classes=100)\n",
    "        # model = preact_resnet164_cifar(num_classes=100)\n",
    "        # model = preact_resnet1001_cifar(num_classes=100)\n",
    "\n",
    "        # model = wide_resnet_cifar(depth=26, width=10, num_classes=100)\n",
    "\n",
    "        # model = resneXt_cifar(depth=29, cardinality=16, baseWidth=64, num_classes=100)\n",
    "        \n",
    "        #model = densenet_BC_cifar(depth=190, k=40, num_classes=100)\n",
    "\n",
    "        # mkdir a new folder to store the checkpoint and best model\n",
    "        if not os.path.exists('result'):\n",
    "            os.makedirs('result')\n",
    "        fdir = 'result/resnet20_cifar10'\n",
    "        if not os.path.exists(fdir):\n",
    "            os.makedirs(fdir)\n",
    "\n",
    "        # adjust the lr according to the model type\n",
    "        if isinstance(model, (ResNet_Cifar, PreAct_ResNet_Cifar)):\n",
    "            model_type = 1\n",
    "        elif isinstance(model, Wide_ResNet_Cifar):\n",
    "            model_type = 2\n",
    "        elif isinstance(model, (ResNeXt_Cifar, DenseNet_Cifar)):\n",
    "            model_type = 3\n",
    "        else:\n",
    "            print('model type unrecognized...')\n",
    "            return\n",
    "\n",
    "        #model = nn.DataParallel(model).cuda()\n",
    "        model = nn.DataParallel(model)\n",
    "        #criterion = nn.CrossEntropyLoss().cuda()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "        cudnn.benchmark = True\n",
    "    else:\n",
    "        print('Cuda is not available!')\n",
    "        return\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print('=> loading checkpoint \"{}\"'.format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec = checkpoint['best_prec']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    # Data loading and preprocessing\n",
    "    # CIFAR10\n",
    "    if args.cifar_type == 10:\n",
    "        print('=> loading cifar10 data...')\n",
    "        normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "        \n",
    "        root_path = \"./data_treatment/reverse_2\"\n",
    "        #root_path = \"./data\"\n",
    "        #train_dataset = torchvision.datasets.CIFAR10(\n",
    "        train_dataset = ImagesDataset(\n",
    "            root=root_path, \n",
    "            train=True, \n",
    "            download=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "        #test_dataset = torchvision.datasets.CIFAR10(\n",
    "        test_dataset = ImagesDataset(\n",
    "            root=root_path,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "    \n",
    "    if args.evaluate:\n",
    "        validate(testloader, model, criterion)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch, model_type)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(trainloader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on test set\n",
    "        prec = validate(testloader, model, criterion)\n",
    "\n",
    "        # remember best precision and save checkpoint\n",
    "        is_best = prec > best_prec\n",
    "        best_prec = max(prec,best_prec)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec': best_prec,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, fdir)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        '''\n",
    "        print(type(input), input.shape)\n",
    "        print(type(target), target, len(target), i)\n",
    "        print(type(input[0][0][0]))\n",
    "        print(type(target[0]))\n",
    "        #input = input.double()\n",
    "        '''\n",
    "        input = input.double()\n",
    "        target = target.long()\n",
    "        \n",
    "        #print(\"Here\", i)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #input, target = input.cuda(), target.cuda()\n",
    "        #input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        model = model.double()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "    \n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            #input, target = input.cuda(), target.cuda()\n",
    "            input = input.double()\n",
    "            target = target.long()\n",
    "\n",
    "            # compute output\n",
    "            model = model.double()\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, model_type):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    if model_type == 1:\n",
    "        if epoch < 80:\n",
    "            lr = args.lr\n",
    "        elif epoch < 120:\n",
    "            lr = args.lr * 0.1\n",
    "        else:\n",
    "            lr = args.lr * 0.01\n",
    "    elif model_type == 2:\n",
    "        if epoch < 60:\n",
    "            lr = args.lr\n",
    "        elif epoch < 120:\n",
    "            lr = args.lr * 0.2\n",
    "        elif epoch < 160:\n",
    "            lr = args.lr * 0.04\n",
    "        else:\n",
    "            lr = args.lr * 0.008\n",
    "    elif model_type == 3:\n",
    "        if epoch < 150:\n",
    "            lr = args.lr\n",
    "        elif epoch < 225:\n",
    "            lr = args.lr * 0.1\n",
    "        else:\n",
    "            lr = args.lr * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "=> loading cifar10 data...\n",
      "(3000, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "Epoch: [0][0/24]\tTime 6.002 (6.002)\tData 0.111 (0.111)\tLoss 2.3627 (2.3627)\tPrec 10.156% (10.156%)\n",
      "Epoch: [0][10/24]\tTime 3.295 (3.583)\tData 0.004 (0.014)\tLoss 2.3131 (2.3589)\tPrec 8.594% (9.446%)\n",
      "Epoch: [0][20/24]\tTime 3.105 (3.405)\tData 0.003 (0.009)\tLoss 2.4662 (2.3771)\tPrec 8.594% (9.449%)\n",
      "Test: [0/2]\tTime 0.840 (0.840)\tLoss 7.6768 (7.6768)\tPrec 8.000% (8.000%)\n",
      " * Prec 10.500% \n",
      "Epoch: [1][0/24]\tTime 3.358 (3.358)\tData 0.055 (0.055)\tLoss 2.4840 (2.4840)\tPrec 10.156% (10.156%)\n",
      "Epoch: [1][10/24]\tTime 3.350 (3.380)\tData 0.004 (0.009)\tLoss 2.3178 (2.3840)\tPrec 10.938% (11.293%)\n",
      "Epoch: [1][20/24]\tTime 3.079 (3.298)\tData 0.003 (0.006)\tLoss 2.3320 (2.3788)\tPrec 8.594% (10.417%)\n",
      "Test: [0/2]\tTime 0.881 (0.881)\tLoss 2.3834 (2.3834)\tPrec 10.000% (10.000%)\n",
      " * Prec 9.500% \n",
      "Epoch: [2][0/24]\tTime 3.999 (3.999)\tData 0.067 (0.067)\tLoss 2.3562 (2.3562)\tPrec 7.812% (7.812%)\n",
      "Epoch: [2][10/24]\tTime 3.077 (3.416)\tData 0.003 (0.009)\tLoss 2.3085 (2.3329)\tPrec 11.719% (10.227%)\n",
      "Epoch: [2][20/24]\tTime 3.209 (3.325)\tData 0.003 (0.007)\tLoss 2.3139 (2.3293)\tPrec 13.281% (10.379%)\n",
      "Test: [0/2]\tTime 0.934 (0.934)\tLoss 2.3334 (2.3334)\tPrec 10.000% (10.000%)\n",
      " * Prec 9.500% \n",
      "Epoch: [3][0/24]\tTime 3.681 (3.681)\tData 0.057 (0.057)\tLoss 2.3156 (2.3156)\tPrec 13.281% (13.281%)\n",
      "Epoch: [3][10/24]\tTime 3.211 (3.292)\tData 0.003 (0.009)\tLoss 2.3683 (2.3336)\tPrec 5.469% (9.020%)\n",
      "Epoch: [3][20/24]\tTime 3.527 (3.277)\tData 0.003 (0.006)\tLoss 2.3224 (2.3296)\tPrec 13.281% (9.226%)\n",
      "Test: [0/2]\tTime 0.895 (0.895)\tLoss 2.2985 (2.2985)\tPrec 11.000% (11.000%)\n",
      " * Prec 9.000% \n",
      "Epoch: [4][0/24]\tTime 3.757 (3.757)\tData 0.053 (0.053)\tLoss 2.2995 (2.2995)\tPrec 9.375% (9.375%)\n",
      "Epoch: [4][10/24]\tTime 3.244 (3.354)\tData 0.004 (0.008)\tLoss 2.3205 (2.3148)\tPrec 7.812% (9.801%)\n",
      "Epoch: [4][20/24]\tTime 3.415 (3.323)\tData 0.004 (0.006)\tLoss 2.3584 (2.3218)\tPrec 10.156% (9.598%)\n",
      "Test: [0/2]\tTime 0.884 (0.884)\tLoss 2.2924 (2.2924)\tPrec 16.000% (16.000%)\n",
      " * Prec 13.000% \n",
      "Epoch: [5][0/24]\tTime 3.528 (3.528)\tData 0.062 (0.062)\tLoss 2.3081 (2.3081)\tPrec 8.594% (8.594%)\n",
      "Epoch: [5][10/24]\tTime 3.073 (3.257)\tData 0.004 (0.009)\tLoss 2.3343 (2.3172)\tPrec 7.031% (8.949%)\n",
      "Epoch: [5][20/24]\tTime 3.166 (3.264)\tData 0.003 (0.006)\tLoss 2.3263 (2.3172)\tPrec 14.062% (9.375%)\n",
      "Test: [0/2]\tTime 0.943 (0.943)\tLoss 2.2868 (2.2868)\tPrec 16.000% (16.000%)\n",
      " * Prec 13.000% \n",
      "Epoch: [6][0/24]\tTime 4.069 (4.069)\tData 0.062 (0.062)\tLoss 2.2978 (2.2978)\tPrec 12.500% (12.500%)\n",
      "Epoch: [6][10/24]\tTime 3.266 (3.406)\tData 0.004 (0.009)\tLoss 2.3218 (2.3101)\tPrec 9.375% (10.369%)\n",
      "Epoch: [6][20/24]\tTime 3.290 (3.371)\tData 0.004 (0.007)\tLoss 2.3091 (2.3093)\tPrec 13.281% (10.863%)\n",
      "Test: [0/2]\tTime 0.858 (0.858)\tLoss 2.3134 (2.3134)\tPrec 11.000% (11.000%)\n",
      " * Prec 10.500% \n",
      "Epoch: [7][0/24]\tTime 3.628 (3.628)\tData 0.060 (0.060)\tLoss 2.3058 (2.3058)\tPrec 9.375% (9.375%)\n",
      "Epoch: [7][10/24]\tTime 3.330 (3.331)\tData 0.004 (0.009)\tLoss 2.3227 (2.3102)\tPrec 7.031% (10.156%)\n",
      "Epoch: [7][20/24]\tTime 3.369 (3.328)\tData 0.003 (0.006)\tLoss 2.3359 (2.3143)\tPrec 6.250% (9.487%)\n",
      "Test: [0/2]\tTime 0.927 (0.927)\tLoss 2.3221 (2.3221)\tPrec 11.000% (11.000%)\n",
      " * Prec 10.500% \n",
      "Epoch: [8][0/24]\tTime 3.801 (3.801)\tData 0.063 (0.063)\tLoss 2.3334 (2.3334)\tPrec 7.031% (7.031%)\n",
      "Epoch: [8][10/24]\tTime 3.526 (3.351)\tData 0.004 (0.009)\tLoss 2.3110 (2.3153)\tPrec 7.812% (9.162%)\n",
      "Epoch: [8][20/24]\tTime 3.133 (3.320)\tData 0.003 (0.007)\tLoss 2.3191 (2.3138)\tPrec 7.812% (9.635%)\n",
      "Test: [0/2]\tTime 0.833 (0.833)\tLoss 2.3101 (2.3101)\tPrec 8.000% (8.000%)\n",
      " * Prec 10.500% \n",
      "Epoch: [9][0/24]\tTime 3.390 (3.390)\tData 0.058 (0.058)\tLoss 2.2933 (2.2933)\tPrec 9.375% (9.375%)\n",
      "Epoch: [9][10/24]\tTime 3.324 (3.248)\tData 0.005 (0.009)\tLoss 2.3161 (2.3123)\tPrec 7.031% (10.440%)\n",
      "Epoch: [9][20/24]\tTime 3.573 (3.288)\tData 0.004 (0.006)\tLoss 2.3077 (2.3103)\tPrec 12.500% (10.528%)\n",
      "Test: [0/2]\tTime 0.922 (0.922)\tLoss 2.2920 (2.2920)\tPrec 16.000% (16.000%)\n",
      " * Prec 13.000% \n"
     ]
    }
   ],
   "source": [
    "#python main.py --epoch 160 --batch-size 128 --lr 0.1 --momentum 0.9 --wd 1e-4 -ct 10\n",
    "main1(epoch=10, batch_size=128, lr=0.1, momentum=0.9, wd=1e-4, ct=100)\n",
    "#main1(epoch=2, batch_size=4096, lr=0.1, momentum=0.9, wd=1e-4, ct=10, print_freq=10)\n",
    "#main()\n",
    "\n",
    "#, axis = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#New part of the code deals with lens blur (or variable radial blur so as to re-train model)\n",
    "#N.B. Ealier cell of the notebook does it only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find depth\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "#help(cv2.StereoBM_create)\n",
    "\n",
    "imgL = cv2.imread('stereo/tsukuba_L.png',0)\n",
    "imgR = cv2.imread('stereo/tsukuba_R.png',0)\n",
    "\n",
    "#stereo = cv2.createStereoBM(numDisparities=16, blockSize=15)\n",
    "stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\n",
    "disparity = stereo.compute(imgL,imgR)\n",
    "plt.imshow(disparity,'gray')\n",
    "plt.show()\n",
    "cv2.imwrite('stereo/tsukuba.png',disparity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "def load_cfar10_batch_new(cifar10_dataset_folder_path, batch_name):\n",
    "    with open(cifar10_dataset_folder_path + batch_name, mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, matplotlib.pyplot as plt, os\n",
    "from PIL import Image\n",
    "\n",
    "#a, b = load_cfar10_batch(\"./data/\", 1)\n",
    "#print(len(b))\n",
    "#plt.imshow(a[1])\n",
    "#plt.save(a[1], '1.jpg')\n",
    "\n",
    "for batch in [1, 2, 3, 4, 5]:\n",
    "    a, b = load_cfar10_batch(\"./data/\", 1)\n",
    "    directory = \"CIFAR_extracted_data_{}\".format(batch)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for idx in range(1, 10001):\n",
    "        img = Image.fromarray(a[1])\n",
    "        img.save('./CIFAR_extracted_data_{}/{}.jpg'.format(batch, idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "misc.imsave('my.jpg', a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "    a, b = load_cfar10_batch_new(\"./data/\", batch)\n",
    "    directory = \"./images/orig/CIFAR_{}\".format(batch)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for idx in range(1, 10001):\n",
    "        img = Image.fromarray(a[1])\n",
    "        img.save('./images/orig/CIFAR_{}/{}.jpg'.format(batch, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfar10_batch_new(cifar10_dataset_folder_path, batch_name):\n",
    "    with open(cifar10_dataset_folder_path + batch_name, mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data']\n",
    "    #print(features.shape, type(features))\n",
    "    features = features.reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    print(features.shape, type(features))\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "    arr, labels = load_cfar10_batch_new(\"./data/\", batch)\n",
    "    directory = \"./images/orig/CIFAR_{}\".format(batch)\n",
    "    \n",
    "    '''\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    '''\n",
    "\n",
    "    for idx in range(1, 10001):\n",
    "        continue\n",
    "        #img = Image.fromarray(a[1])\n",
    "        #img.save('./images/orig/CIFAR_{}/{}.jpg'.format(batch, idx))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "im = np.array(Image.open('1.jpg'))\n",
    "print(im.shape, im.dtype)\n",
    "im1 = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "    print(batch)\n",
    "    arr, labels = load_cfar10_batch_new(\"./data/\", batch)\n",
    "    directory = \"./images/orig/CIFAR_{}\".format(batch)\n",
    "    \n",
    "    data_new = {}\n",
    "    data_new['data'] = np.empty([10000, 3072])\n",
    "    data_new['labels'] = labels\n",
    "    \n",
    "    for idx in range(1, 10001):\n",
    "        im = Image.open('./images/orig/CIFAR_{}/{}.jpg'.format(batch, idx))\n",
    "        im = (np.array(im))\n",
    "        r = im[:,:,0].flatten()\n",
    "        g = im[:,:,1].flatten()\n",
    "        b = im[:,:,2].flatten()\n",
    "        \n",
    "        data_new['data'][idx - 1] = np.array(list(r) + list(g) + list(b), dtype=np.uint8)\n",
    "    \n",
    "    print(data_new['data'].shape, type(data_new['data']))\n",
    "    \n",
    "    directory_new = \"./data_new/cifar-10-batches-py/\"\n",
    "    if not os.path.exists(directory_new):\n",
    "        os.makedirs(directory_new)\n",
    "        \n",
    "    with open(directory_new + batch, 'wb') as file:\n",
    "        pickle.dump(data_new, file)\n",
    "\n",
    "    with open(directory_new + batch, 'rb') as file:\n",
    "        data_new_rd = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    #print(data_new['data'] == data_new_rd['data'], data_new['labels'] == data_new_rd['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "    print(batch)\n",
    "    arr, labels = load_cfar10_batch_new(\"./data/\", batch)\n",
    "    arr, labels = load_cfar10_batch_new(\"./data_new/\", batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "    arr, labels = load_cfar10_batch_new(\"./data/\", batch)\n",
    "    directory = \"./images/orig/CIFAR_{}\".format(batch)\n",
    "    \n",
    "    '''\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    '''\n",
    "\n",
    "    for idx in range(1, 10001):\n",
    "        continue\n",
    "        im = Image.open('./images/orig/CIFAR_{}/{}.jpg'.format(batch, idx))\n",
    "        im = (np.array(im))\n",
    "        \n",
    "        \n",
    "im = Image.open('1.jpg')\n",
    "im = (np.array(im))\n",
    "\n",
    "r = im[:,:,0].flatten()\n",
    "g = im[:,:,1].flatten()\n",
    "b = im[:,:,2].flatten()\n",
    "#label = [1]\n",
    "data_new = {}\n",
    "data_new['data'] = np.empty([10000, 3072])\n",
    "print(data_new['data'].shape, type(data_new['data']))\n",
    "\n",
    "#out = np.array(list(label) + list(r) + list(g) + list(b),np.uint8)\n",
    "data_new['data'][0] = np.array(list(r) + list(g) + list(b), dtype=np.uint8)\n",
    "print(data_new['data'].shape, type(data_new['data']))\n",
    "print(data_new['data'][0].shape)\n",
    "#out.tofile(\"out.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pickle, os\n",
    "from PIL import Image\n",
    "\n",
    "def load_cfar10_batch_new(cifar10_dataset_folder_path, batch_name):\n",
    "    with open(cifar10_dataset_folder_path + batch_name, mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data']\n",
    "    #print(features.shape, type(features))\n",
    "    features = features.reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    print(features.shape, type(features))\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "for method in ['reverse']:\n",
    "    for route in range(2, 3):\n",
    "        \n",
    "        for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "            arr, labels = load_cfar10_batch_new(\"./data/\", batch)\n",
    "            directory = \"./images/new/CIFAR_{}_{}_{}\".format(batch, method, route)\n",
    "            print(directory)\n",
    "\n",
    "            #'''\n",
    "            data_new = {}\n",
    "            data_new['data'] = np.empty([10000, 3072])\n",
    "            data_new['labels'] = labels\n",
    "            print(len(labels))\n",
    "\n",
    "            for idx in range(1, 10001):\n",
    "                im = Image.open(\"{}/{}.jpg\".format(directory, idx))\n",
    "                #./images/orig/CIFAR_{}/{}.jpg'.format(batch, idx))\n",
    "                im = (np.array(im))\n",
    "                r = im[:,:,0].flatten()\n",
    "                g = im[:,:,1].flatten()\n",
    "                b = im[:,:,2].flatten()\n",
    "\n",
    "                data_new['data'][idx - 1] = np.array(list(r) + list(g) + list(b), dtype=np.uint8)\n",
    "                #print(data_new['data'][idx - 1].shape)\n",
    "                #data_new['data'][idx - 1] = np.array(list(r) + list(g) + list(b), axis=1)\n",
    "                #print(np.concatenate((list(r), list(g), list(b)), axis = 0).shape)\n",
    "                data_new['data'][idx - 1] = np.concatenate((list(r), list(g), list(b)), axis = 0)\n",
    "                #print(data_new['data'][idx - 1].shape)\n",
    "\n",
    "            print(data_new['data'].shape, type(data_new['data']))\n",
    "\n",
    "            #'''\n",
    "            directory_new = \"./data_treatment/{}_{}/cifar-10-batches-py/\".format(method, route)\n",
    "            if not os.path.exists(directory_new):\n",
    "                os.makedirs(directory_new)\n",
    "\n",
    "            #'''\n",
    "            with open(directory_new + batch, 'wb') as file:\n",
    "                pickle.dump(data_new, file)\n",
    "            #'''\n",
    "                \n",
    "            #'''\n",
    "            \n",
    "            #'''\n",
    "            with open(directory_new + batch, 'rb') as file:\n",
    "                data_new_rd = pickle.load(file, encoding='latin1')\n",
    "            #'''\n",
    "\n",
    "            #print(data_new['data'] == data_new_rd['data'], data_new['labels'] == data_new_rd['labels'])\n",
    "            #print(data_new['data'] == data_new_rd['data'])\n",
    "            #print(data_new['labels'] == data_new_rd['labels'])\n",
    "            #break\n",
    "        #break\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['reverse']:\n",
    "    data_all = np.array([], dtype=np.int64).reshape(0, 32, 32, 3)\n",
    "    labels_all = []\n",
    "\n",
    "    for route in range(2, 3):\n",
    "        for batch in ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']:\n",
    "            arr, labels = load_cfar10_batch_new(\"./data/\", batch)\n",
    "            labels_all = labels_all + labels\n",
    "            print(arr.shape)\n",
    "            data_all = np.vstack((data_all, arr))\n",
    "            \n",
    "            print(len(labels_all), data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
